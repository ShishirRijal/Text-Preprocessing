{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"Text preprocessing is a crucial step in Natural Language Processing (NLP) that involves transforming raw text into a format suitable for analysis. This process helps improve the performance of machine learning models by reducing noise and ensuring that the data is consistent and relevant.","metadata":{}},{"cell_type":"markdown","source":"# Load data\nLet's use the imdb movies review dataset to learn the text preprocessing. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.383488Z","iopub.status.idle":"2024-09-21T11:57:38.383870Z","shell.execute_reply.started":"2024-09-21T11:57:38.383677Z","shell.execute_reply":"2024-09-21T11:57:38.383695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.384884Z","iopub.status.idle":"2024-09-21T11:57:38.385265Z","shell.execute_reply.started":"2024-09-21T11:57:38.385085Z","shell.execute_reply":"2024-09-21T11:57:38.385104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.386660Z","iopub.status.idle":"2024-09-21T11:57:38.387051Z","shell.execute_reply.started":"2024-09-21T11:57:38.386839Z","shell.execute_reply":"2024-09-21T11:57:38.386857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.388715Z","iopub.status.idle":"2024-09-21T11:57:38.389147Z","shell.execute_reply.started":"2024-09-21T11:57:38.388926Z","shell.execute_reply":"2024-09-21T11:57:38.388947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Lowercasing","metadata":{}},{"cell_type":"markdown","source":"The primary goal is to convert all characters in the text to lowercase, ensuring uniformity and consistency throughout the dataset. This step helps eliminate variations caused by differences in capitalization, which can lead to the same words being treated as distinct tokens due to the case sensitive nature of languages like Python.\n\nIt ensures that words like \"Apple,\" \"apple,\" and \"APPLE\" are treated as the same word","metadata":{}},{"cell_type":"code","source":"# Sample text\ntext = \"Hello World! I'm learning text preprocessing.\"\n\n# Convert to lowercase\nlowercase_text = text.lower()\n\nprint(lowercase_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.390376Z","iopub.status.idle":"2024-09-21T11:57:38.390759Z","shell.execute_reply.started":"2024-09-21T11:57:38.390567Z","shell.execute_reply":"2024-09-21T11:57:38.390587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review'][45]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.392188Z","iopub.status.idle":"2024-09-21T11:57:38.392699Z","shell.execute_reply.started":"2024-09-21T11:57:38.392427Z","shell.execute_reply":"2024-09-21T11:57:38.392456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.393952Z","iopub.status.idle":"2024-09-21T11:57:38.394336Z","shell.execute_reply.started":"2024-09-21T11:57:38.394148Z","shell.execute_reply":"2024-09-21T11:57:38.394168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.395508Z","iopub.status.idle":"2024-09-21T11:57:38.395859Z","shell.execute_reply.started":"2024-09-21T11:57:38.395681Z","shell.execute_reply":"2024-09-21T11:57:38.395699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Remove HTML Tags","metadata":{}},{"cell_type":"markdown","source":"In many real-world applications, text data is often extracted from web pages, which can contain HTML tags. These tags are useful for structuring content but are irrelevant when processing text for NLP tasks. Removing HTML tags is, therefore, a key preprocessing step to clean the data and make it usable for text analysis.\n\nHTML tags,don’t provide any useful information for text analysis and can interfere with the accuracy of NLP models.\n\nBetter Focus on Content: By removing these tags, we focus only on the actual text content, which helps the model capture meaningful information.","metadata":{}},{"cell_type":"markdown","source":"Here’s an example of raw text with HTML tags:\n\n<p>Hello, <strong>world!</strong> Welcome to <a href='https://example.com'>NLP</a> tutorials.</p>\n","metadata":{}},{"cell_type":"code","source":"import re\n\ndef remove_html_tags(text):\n    # Regular expression pattern to match HTML tags\n    pattern = re.compile('<.*?>')\n    \n    # Replace the HTML tags with an empty string\n    return re.sub(pattern, '', text)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.397146Z","iopub.status.idle":"2024-09-21T11:57:38.397495Z","shell.execute_reply.started":"2024-09-21T11:57:38.397317Z","shell.execute_reply":"2024-09-21T11:57:38.397335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"html_text = \"<p>Hello, <strong>world!</strong> Welcome to <a href='https://example.com'>NLP</a> tutorials.</p>\"\nhtml_text","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.398458Z","iopub.status.idle":"2024-09-21T11:57:38.398797Z","shell.execute_reply.started":"2024-09-21T11:57:38.398622Z","shell.execute_reply":"2024-09-21T11:57:38.398640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_text = remove_html_tags(html_text)\nprint(cleaned_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.400192Z","iopub.status.idle":"2024-09-21T11:57:38.400543Z","shell.execute_reply.started":"2024-09-21T11:57:38.400365Z","shell.execute_reply":"2024-09-21T11:57:38.400383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review'] = df['review'].apply(remove_html_tags)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.402038Z","iopub.status.idle":"2024-09-21T11:57:38.402416Z","shell.execute_reply.started":"2024-09-21T11:57:38.402226Z","shell.execute_reply":"2024-09-21T11:57:38.402246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review']","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.406125Z","iopub.status.idle":"2024-09-21T11:57:38.406541Z","shell.execute_reply.started":"2024-09-21T11:57:38.406342Z","shell.execute_reply":"2024-09-21T11:57:38.406364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Remove URLs","metadata":{}},{"cell_type":"markdown","source":"URLs, while essential for linking content on the web, typically don't contribute meaningful linguistic information for most NLP tasks, such as text classification, sentiment analysis, or language modeling. \n\nInstead, they add noise and can confuse machine learning models. By removing URLs, we help the model focus on the core text, making it easier to extract patterns and meaning. \nIn most cases, removing URLs enhances the overall quality and performance of NLP models, unless the task specifically requires analyzing hyperlinks.","metadata":{}},{"cell_type":"code","source":"import re\n\ndef remove_urls(text):\n    # Regular expression pattern to match URLs\n    url_pattern = re.compile(r'http[s]?://\\S+|www\\.\\S+')\n    \n    # Replace URLs with an empty string\n    return re.sub(url_pattern, '', text)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.407859Z","iopub.status.idle":"2024-09-21T11:57:38.408255Z","shell.execute_reply.started":"2024-09-21T11:57:38.408072Z","shell.execute_reply":"2024-09-21T11:57:38.408091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_with_urls = \"Check out my portfolio at https://example.com or visit www.example.org for more details.\"\nprint(text_with_urls)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.409201Z","iopub.status.idle":"2024-09-21T11:57:38.409569Z","shell.execute_reply.started":"2024-09-21T11:57:38.409382Z","shell.execute_reply":"2024-09-21T11:57:38.409401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_text = remove_urls(text_with_urls)\nprint(cleaned_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.411381Z","iopub.status.idle":"2024-09-21T11:57:38.411929Z","shell.execute_reply.started":"2024-09-21T11:57:38.411636Z","shell.execute_reply":"2024-09-21T11:57:38.411664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if there are any links in our dataset, let's remove that\ndf['review'] = df['review'].apply(remove_urls)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.413316Z","iopub.status.idle":"2024-09-21T11:57:38.413824Z","shell.execute_reply.started":"2024-09-21T11:57:38.413554Z","shell.execute_reply":"2024-09-21T11:57:38.413582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review']","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.415495Z","iopub.status.idle":"2024-09-21T11:57:38.416047Z","shell.execute_reply.started":"2024-09-21T11:57:38.415738Z","shell.execute_reply":"2024-09-21T11:57:38.415766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Remove Punctuation","metadata":{}},{"cell_type":"markdown","source":"Punctuation is often irrelevant in many NLP tasks, so removing it helps in simplifying the text. \n\nRemoving punctuation during text preprocessing is important primarily because of its impact on tokenization. \n\nPunctuation marks, while important for human readability, generally do not carry significant meaning in most NLP tasks. \nWhen left in the text, punctuation can cause issues during tokenization, leading to an unnecessarily large or fragmented vocabulary. \nFor instance, \"hello!\" and \"hello\" would be treated as different tokens if punctuation is not removed, despite having the same meaning.","metadata":{}},{"cell_type":"code","source":"import string\nstring.punctuation # are the python punctuations","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.571083Z","iopub.execute_input":"2024-09-21T11:57:38.572018Z","iopub.status.idle":"2024-09-21T11:57:38.579702Z","shell.execute_reply.started":"2024-09-21T11:57:38.571966Z","shell.execute_reply":"2024-09-21T11:57:38.578347Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"},"metadata":{}}]},{"cell_type":"code","source":"def remove_punctuation(text):\n    return text.translate(str.maketrans('', '', string.punctuation))","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:38.946737Z","iopub.execute_input":"2024-09-21T11:57:38.947697Z","iopub.status.idle":"2024-09-21T11:57:38.952803Z","shell.execute_reply.started":"2024-09-21T11:57:38.947645Z","shell.execute_reply":"2024-09-21T11:57:38.951706Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Example\nenglish_text = \"Hello, world! How's everything going?\"\ncleaned_text = remove_punctuation(english_text)\nprint(cleaned_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:39.119401Z","iopub.execute_input":"2024-09-21T11:57:39.119803Z","iopub.status.idle":"2024-09-21T11:57:39.125782Z","shell.execute_reply.started":"2024-09-21T11:57:39.119756Z","shell.execute_reply":"2024-09-21T11:57:39.124491Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Hello world Hows everything going\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\n\ndef remove_nepali_punctuation(text):\n    # Defining both English and Nepali punctuation marks\n    nepali_punctuation = r'[।,!?–—\"\\'\\‘\\’\\“\\”\\(\\)\\[\\]\\{\\}:;]'\n    \n    # Use regex to remove the defined punctuation marks\n    return re.sub(nepali_punctuation, '', text)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:39.292764Z","iopub.execute_input":"2024-09-21T11:57:39.293208Z","iopub.status.idle":"2024-09-21T11:57:39.299410Z","shell.execute_reply.started":"2024-09-21T11:57:39.293166Z","shell.execute_reply":"2024-09-21T11:57:39.297983Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Example\nnepali_text = \"यो एक परीक्षण वाक्य हो। के तपाईँलाई थाहा छ?\"\ncleaned_nepali_text = remove_nepali_punctuation(nepali_text)\nprint(cleaned_nepali_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:39.465812Z","iopub.execute_input":"2024-09-21T11:57:39.466257Z","iopub.status.idle":"2024-09-21T11:57:39.473331Z","shell.execute_reply.started":"2024-09-21T11:57:39.466220Z","shell.execute_reply":"2024-09-21T11:57:39.471993Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"यो एक परीक्षण वाक्य हो के तपाईँलाई थाहा छ\n","output_type":"stream"}]},{"cell_type":"code","source":"# Applying this in our dataset\ndf['review'] = df['review'].apply(remove_punctuation)\ndf['review']","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:39.636106Z","iopub.execute_input":"2024-09-21T11:57:39.636811Z","iopub.status.idle":"2024-09-21T11:57:39.664331Z","shell.execute_reply.started":"2024-09-21T11:57:39.636753Z","shell.execute_reply":"2024-09-21T11:57:39.662683Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Applying this in our dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(remove_punctuation)\n\u001b[1;32m      3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"],"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# 5. Chat word treatment","metadata":{}},{"cell_type":"markdown","source":"In Natural Language Processing (NLP) tasks involving chat data, handling chat-specific language is crucial for accurate analysis. Chat messages are typically informal, often filled with abbreviations, slang, emotions, inconsistent grammar, and punctuation variations. Properly treating these \"chat words\" is an important preprocessing step.\n\nHere are some common challenges and strategies for dealing with chat words:\n\nAbbreviations and Slang:\n\nChat data often includes abbreviations like \"u\" for \"you,\" \"lol\" for \"laugh out loud,\" or \"omg\" for \"oh my god.\"\nSolution: Use a mapping or a dictionary to expand common abbreviations to their full forms.\n\n\nMisspellings and Typos:\n\nChat text often contains misspellings due to fast typing, like \"thnks\" instead of \"thanks.\"\nSolution: Apply spelling correction techniques or use models trained to handle noisy text.\n\nRepetitions:\n\nUsers often exaggerate for emphasis, such as \"soooo good\" or \"noooo way.\"\nSolution: Normalize repeated characters (e.g., reducing \"soooo\" to \"so\").\n","metadata":{}},{"cell_type":"code","source":"#source: https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt\nchat_abbreviations = {\n    \"AFAIK\": \"As Far As I Know\",\n    \"AFK\": \"Away From Keyboard\",\n    \"ASAP\": \"As Soon As Possible\",\n    \"ATK\": \"At The Keyboard\",\n    \"ATM\": \"At The Moment\",\n    \"A3\": \"Anytime, Anywhere, Anyplace\",\n    \"BAK\": \"Back At Keyboard\",\n    \"BBL\": \"Be Back Later\",\n    \"BBS\": \"Be Back Soon\",\n    \"BFN\": \"Bye For Now\",\n    \"B4N\": \"Bye For Now\",\n    \"BRB\": \"Be Right Back\",\n    \"BRT\": \"Be Right There\",\n    \"BTW\": \"By The Way\",\n    \"B4\": \"Before\",\n    \"CU\": \"See You\",\n    \"CUL8R\": \"See You Later\",\n    \"CYA\": \"See You\",\n    \"FAQ\": \"Frequently Asked Questions\",\n    \"FC\": \"Fingers Crossed\",\n    \"FWIW\": \"For What It's Worth\",\n    \"FYI\": \"For Your Information\",\n    \"GAL\": \"Get A Life\",\n    \"GG\": \"Good Game\",\n    \"GN\": \"Good Night\",\n    \"GMTA\": \"Great Minds Think Alike\",\n    \"GR8\": \"Great!\",\n    \"G9\": \"Genius\",\n    \"IC\": \"I See\",\n    \"ICQ\": \"I Seek You (also a chat program)\",\n    \"ILU\": \"I Love You\",\n    \"IMHO\": \"In My Honest/Humble Opinion\",\n    \"IMO\": \"In My Opinion\",\n    \"IOW\": \"In Other Words\",\n    \"IRL\": \"In Real Life\",\n    \"KISS\": \"Keep It Simple, Stupid\",\n    \"LDR\": \"Long Distance Relationship\",\n    \"LMAO\": \"Laugh My A.. Off\",\n    \"LOL\": \"Laughing Out Loud\",\n    \"LTNS\": \"Long Time No See\",\n    \"L8R\": \"Later\",\n    \"MTE\": \"My Thoughts Exactly\",\n    \"M8\": \"Mate\",\n    \"NRN\": \"No Reply Necessary\",\n    \"OIC\": \"Oh I See\",\n    \"PITA\": \"Pain In The A..\",\n    \"PRT\": \"Party\",\n    \"PRW\": \"Parents Are Watching\",\n    \"QPSA\": \"Que Pasa?\",\n    \"ROFL\": \"Rolling On The Floor Laughing\",\n    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n    \"SK8\": \"Skate\",\n    \"STATS\": \"Your Sex and Age\",\n    \"ASL\": \"Age, Sex, Location\",\n    \"THX\": \"Thank You\",\n    \"TTFN\": \"Ta-Ta For Now!\",\n    \"TTYL\": \"Talk To You Later\",\n    \"U\": \"You\",\n    \"U2\": \"You Too\",\n    \"U4E\": \"Yours For Ever\",\n    \"WB\": \"Welcome Back\",\n    \"WTF\": \"What The F...\",\n    \"WTG\": \"Way To Go!\",\n    \"WUF\": \"Where Are You From?\",\n    \"W8\": \"Wait...\",\n    \"7K\": \"Sick:-D Laugher\",\n    \"TFW\": \"That Feeling When\",\n    \"MFW\": \"My Face When\",\n    \"MRW\": \"My Reaction When\",\n    \"IFYP\": \"I Feel Your Pain\",\n    \"LOL\": \"Laughing Out Loud\",\n    \"TNTL\": \"Trying Not To Laugh\",\n    \"JK\": \"Just Kidding\",\n    \"IDC\": \"I Don’t Care\",\n    \"ILY\": \"I Love You\",\n    \"IMU\": \"I Miss You\",\n    \"ADIH\": \"Another Day In Hell\",\n    \"ZZZ\": \"Sleeping, Bored, Tired\",\n    \"WYWH\": \"Wish You Were Here\",\n    \"TIME\": \"Tears In My Eyes\",\n    \"BAE\": \"Before Anyone Else\",\n    \"FIMH\": \"Forever In My Heart\",\n    \"BSAAW\": \"Big Smile And A Wink\",\n    \"BWL\": \"Bursting With Laughter\",\n    \"LMAO\": \"Laughing My A** Off\",\n    \"BFF\": \"Best Friends Forever\",\n    \"CSL\": \"Can’t Stop Laughing\"\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-21T11:57:40.162282Z","iopub.execute_input":"2024-09-21T11:57:40.162800Z","iopub.status.idle":"2024-09-21T11:57:40.178076Z","shell.execute_reply.started":"2024-09-21T11:57:40.162748Z","shell.execute_reply":"2024-09-21T11:57:40.176866Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"typos_corrections = {\n    \"teh\": \"the\",\n    \"recieve\": \"receive\",\n    \"occuring\": \"occurring\",\n    \"adress\": \"address\",\n    \"tommorow\": \"tomorrow\",\n    \"becuase\": \"because\",\n    \"definately\": \"definitely\",\n    \"seperate\": \"separate\",\n    \"untill\": \"until\",\n    \"embarass\": \"embarrass\",\n    \"neccessary\": \"necessary\",\n    \"wich\": \"which\",\n    \"thier\": \"their\",\n    \"wierd\": \"weird\",\n    \"affect\": \"effect\",\n    \"loose\": \"lose\",\n    \"alot\": \"a lot\",\n    \"suprise\": \"surprise\",\n    \"occured\": \"occurred\",\n    \"accomodate\": \"accommodate\",\n    \"wierd\": \"weird\",\n    \"tehy\": \"they\",\n    \"bruh\": \"brother\",\n    \"beleive\": \"believe\",\n    \"enviroment\": \"environment\",\n    \"definately\": \"definitely\",\n    \"restraunt\": \"restaurant\"\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:20:11.796808Z","iopub.execute_input":"2024-09-21T12:20:11.797267Z","iopub.status.idle":"2024-09-21T12:20:11.804551Z","shell.execute_reply.started":"2024-09-21T12:20:11.797226Z","shell.execute_reply":"2024-09-21T12:20:11.803198Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":" # Common Chat Repetitions\n    \nchat_repetitions = {\n    \"soooo\": \"so\",\n    \"heyyyy\": \"hey\",\n    \"yesss\": \"yes\",\n    \"noooo\": \"no\",\n    \"woooowwwww\": \"wow\",\n    \"whyyy\": \"why\",\n    \"cyaaa\": \"cya\",\n    \"hiii\": \"hi\",\n    \"okkk\": \"ok\",\n    \"loool\": \"lol\",\n    \"yaaa\": \"yeah\",\n    \"bff\": \"best friends forever\"\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:20:11.973870Z","iopub.execute_input":"2024-09-21T12:20:11.974312Z","iopub.status.idle":"2024-09-21T12:20:11.981177Z","shell.execute_reply.started":"2024-09-21T12:20:11.974272Z","shell.execute_reply":"2024-09-21T12:20:11.979529Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"def normalize_repetitions(text):\n    # Reduce repeated characters to two characters max (e.g., \"soooo\" -> \"so\")\n    return re.sub(r'(.)\\1+', r'\\1\\1', text)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:20:12.166128Z","iopub.execute_input":"2024-09-21T12:20:12.166565Z","iopub.status.idle":"2024-09-21T12:20:12.174916Z","shell.execute_reply.started":"2024-09-21T12:20:12.166525Z","shell.execute_reply":"2024-09-21T12:20:12.173673Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"def correct_typos(text, typos_dict):\n    \"\"\"Correct typos based on the dictionary.\"\"\"\n    for typo, correct in typos_dict.items():\n        text = re.sub(r'\\b' + typo + r'\\b', correct, text)  # Replace whole words\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:20:12.316918Z","iopub.execute_input":"2024-09-21T12:20:12.317365Z","iopub.status.idle":"2024-09-21T12:20:12.324009Z","shell.execute_reply.started":"2024-09-21T12:20:12.317324Z","shell.execute_reply":"2024-09-21T12:20:12.322621Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"chat_processing_dict = {**chat_abbreviations, **chat_repetitions, **typos_corrections}\n\ndef preprocess_chat(text):\n    new_text = []\n    \n    # Convert dictionary keys to lowercase for consistent lookup\n    chat_processing_dict_lower = {k.lower(): v for k, v in chat_processing_dict.items()}\n    # print(chat_processing_dict_lower)\n    for word in text.split():\n        # Convert the word to lowercase for case-insensitive matching\n        if word.lower() in chat_processing_dict_lower:\n            new_text.append(chat_processing_dict_lower[word.lower()])\n        else:\n            new_text.append(word)\n    \n    return \" \".join(new_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:22:14.579294Z","iopub.execute_input":"2024-09-21T12:22:14.580186Z","iopub.status.idle":"2024-09-21T12:22:14.586651Z","shell.execute_reply.started":"2024-09-21T12:22:14.580137Z","shell.execute_reply":"2024-09-21T12:22:14.585587Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"\n# Example usage\nexample_chat = \"heyyyy bruh come home ASAP\"\nprocessed_chat = preprocess_chat(example_chat)\nprint(processed_chat)  # Output: \"hey the be right back, ill cya later!!! lol\"","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:22:20.700875Z","iopub.execute_input":"2024-09-21T12:22:20.702020Z","iopub.status.idle":"2024-09-21T12:22:20.707426Z","shell.execute_reply.started":"2024-09-21T12:22:20.701965Z","shell.execute_reply":"2024-09-21T12:22:20.706301Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"hey brother come home As Soon As Possible\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 6. Spelling Correction","metadata":{}},{"cell_type":"markdown","source":"There are various libraries for spell correction. But they work only for the common words. \nWhile working with the regional language and some specific domain, it's better to create our own spell checker. ","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\n\n\nsentence = \"seeveral ggenerations of late king's family aare ddestroyed in the saame mannner\"\ntextBlob = TextBlob(sentence)\ntextBlob.correct().string","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:27:14.022906Z","iopub.execute_input":"2024-09-21T12:27:14.023311Z","iopub.status.idle":"2024-09-21T12:27:14.035255Z","shell.execute_reply.started":"2024-09-21T12:27:14.023274Z","shell.execute_reply":"2024-09-21T12:27:14.033883Z"},"trusted":true},"execution_count":98,"outputs":[{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"\"several generations of late king's family are destroyed in the same manner\""},"metadata":{}}]},{"cell_type":"markdown","source":"# 7. Removing Stop words","metadata":{}},{"cell_type":"markdown","source":"Stopwords are common words like \"is,\" \"the,\" \"and,\" or \"to\" that don't carry significant meaning and often occur frequently in text. In natural language processing (NLP) tasks such as sentiment analysis, document classification etc., removing stopwords reduces noise and thus enhance model performance. \n\nHowever, it in tasks lik POS it should not be removed. \n\n","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\nprint(\"---\\nEnglish Stopwords\\n----\")\nprint(stopwords.words('english'))\nprint(\"---\\nNepali Stopwords\\n----\")\nprint(stopwords.words('nepali'))","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:31:35.306867Z","iopub.execute_input":"2024-09-21T12:31:35.307905Z","iopub.status.idle":"2024-09-21T12:31:35.315132Z","shell.execute_reply.started":"2024-09-21T12:31:35.307812Z","shell.execute_reply":"2024-09-21T12:31:35.313974Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stdout","text":"---\nEnglish Stopwords\n----\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n---\nNepali Stopwords\n----\n['छ', 'र', 'पनि', 'छन्', 'लागि', 'भएको', 'गरेको', 'भने', 'गर्न', 'गर्ने', 'हो', 'तथा', 'यो', 'रहेको', 'उनले', 'थियो', 'हुने', 'गरेका', 'थिए', 'गर्दै', 'तर', 'नै', 'को', 'मा', 'हुन्', 'भन्ने', 'हुन', 'गरी', 'त', 'हुन्छ', 'अब', 'के', 'रहेका', 'गरेर', 'छैन', 'दिए', 'भए', 'यस', 'ले', 'गर्नु', 'औं', 'सो', 'त्यो', 'कि', 'जुन', 'यी', 'का', 'गरि', 'ती', 'न', 'छु', 'छौं', 'लाई', 'नि', 'उप', 'अक्सर', 'आदि', 'कसरी', 'क्रमशः', 'चाले', 'अगाडी', 'अझै', 'अनुसार', 'अन्तर्गत', 'अन्य', 'अन्यत्र', 'अन्यथा', 'अरु', 'अरुलाई', 'अर्को', 'अर्थात', 'अर्थात्', 'अलग', 'आए', 'आजको', 'ओठ', 'आत्म', 'आफू', 'आफूलाई', 'आफ्नै', 'आफ्नो', 'आयो', 'उदाहरण', 'उनको', 'उहालाई', 'एउटै', 'एक', 'एकदम', 'कतै', 'कम से कम', 'कसै', 'कसैले', 'कहाँबाट', 'कहिलेकाहीं', 'का', 'किन', 'किनभने', 'कुनै', 'कुरा', 'कृपया', 'केही', 'कोही', 'गए', 'गरौं', 'गर्छ', 'गर्छु', 'गर्नुपर्छ', 'गयौ', 'गैर', 'चार', 'चाहनुहुन्छ', 'चाहन्छु', 'चाहिए', 'छू', 'जताततै', 'जब', 'जबकि', 'जसको', 'जसबाट', 'जसमा', 'जसलाई', 'जसले', 'जस्तै', 'जस्तो', 'जस्तोसुकै', 'जहाँ', 'जान', 'जाहिर', 'जे', 'जो', 'ठीक', 'तत्काल', 'तदनुसार', 'तपाईको', 'तपाई', 'पर्याप्त', 'पहिले', 'पहिलो', 'पहिल्यै', 'पाँच', 'पाँचौं', 'तल', 'तापनी', 'तिनी', 'तिनीहरू', 'तिनीहरुको', 'तिनिहरुलाई', 'तिमी', 'तिर', 'तीन', 'तुरुन्तै', 'तेस्रो', 'तेस्कारण', 'पूर्व', 'प्रति', 'प्रतेक', 'प्लस', 'फेरी', 'बने', 'त्सपछि', 'त्सैले', 'त्यहाँ', 'थिएन', 'दिनुभएको', 'दिनुहुन्छ', 'दुई', 'देखि', 'बरु', 'बारे', 'बाहिर', 'देखिन्छ', 'देखियो', 'देखे', 'देखेको', 'देखेर', 'दोस्रो', 'धेरै', 'नजिकै', 'नत्र', 'नयाँ', 'निम्ति', 'बाहेक', 'बीच', 'बीचमा', 'भन', 'निम्न', 'निम्नानुसार', 'निर्दिष्ट', 'नौ', 'पक्का', 'पक्कै', 'पछि', 'पछिल्लो', 'पटक', 'पर्छ', 'पर्थ्यो', 'भन्छन्', 'भन्', 'भन्छु', 'भन्दा', 'भन्नुभयो', 'भर', 'भित्र', 'भित्री', 'म', 'मलाई', 'मात्र', 'माथि', 'मुख्य', 'मेरो', 'यति', 'यथोचित', 'यदि', 'यद्यपि', 'यसको', 'यसपछि', 'यसबाहेक', 'यसरी', 'यसो', 'यस्तो', 'यहाँ', 'यहाँसम्म', 'या', 'रही', 'राखे', 'राख्छ', 'राम्रो', 'रूप', 'लगभग', 'वरीपरी', 'वास्तवमा', 'बिरुद्ध', 'बिशेष', 'सायद', 'शायद', 'संग', 'संगै', 'सक्छ', 'सट्टा', 'सधै', 'सबै', 'सबैलाई', 'समय', 'सम्भव', 'सम्म', 'सही', 'साँच्चै', 'सात', 'साथ', 'साथै', 'सारा', 'सोही', 'स्पष्ट', 'हरे', 'हरेक']\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:32:59.422783Z","iopub.execute_input":"2024-09-21T12:32:59.423864Z","iopub.status.idle":"2024-09-21T12:32:59.432263Z","shell.execute_reply.started":"2024-09-21T12:32:59.423782Z","shell.execute_reply":"2024-09-21T12:32:59.431203Z"},"trusted":true},"execution_count":109,"outputs":[{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"['hi', 'there', 'how', 'are', 'you!']"},"metadata":{}}]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\n\ndef remove_stopwords(text):\n    stop_words = stopwords.words(\"english\")\n    filtered_words = [word for word in text.split() if word.lower() not in stop_words]\n    return ' '.join(filtered_words)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:36:30.028909Z","iopub.execute_input":"2024-09-21T12:36:30.029337Z","iopub.status.idle":"2024-09-21T12:36:30.035214Z","shell.execute_reply.started":"2024-09-21T12:36:30.029296Z","shell.execute_reply":"2024-09-21T12:36:30.034107Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"text = \"This is a simple example to demonstrate removing stopwords from a text\"\ncleaned_text = remove_stopwords(text)\nprint(cleaned_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:36:50.162476Z","iopub.execute_input":"2024-09-21T12:36:50.162907Z","iopub.status.idle":"2024-09-21T12:36:50.169822Z","shell.execute_reply.started":"2024-09-21T12:36:50.162852Z","shell.execute_reply":"2024-09-21T12:36:50.168600Z"},"trusted":true},"execution_count":118,"outputs":[{"name":"stdout","text":"simple example demonstrate removing stopwords text\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"I will go to the park in the evening\"\ncleaned_text = remove_stopwords(text)\nprint(cleaned_text) ","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:37:00.608604Z","iopub.execute_input":"2024-09-21T12:37:00.609571Z","iopub.status.idle":"2024-09-21T12:37:00.615603Z","shell.execute_reply.started":"2024-09-21T12:37:00.609523Z","shell.execute_reply":"2024-09-21T12:37:00.614440Z"},"trusted":true},"execution_count":120,"outputs":[{"name":"stdout","text":"go park evening\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 8. Handling Emojis","metadata":{}},{"cell_type":"markdown","source":"Handling emojis in text is important because emojis convey sentiment, emotions, and sometimes additional context in communication. \n\nDepending on your use case, you can choose to either remove, replace, or convert emojis.\n\n","metadata":{}},{"cell_type":"code","source":"import re\n\ndef remove_emojis(text):\n    # Unicode range for emojis\n    emoji_pattern = re.compile(\n        \"[\"\n        \"\\U0001F600-\\U0001F64F\"  # emoticons\n        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        \"\\U00002702-\\U000027B0\"  # other symbols\n        \"\\U000024C2-\\U0001F251\"\n        \"]+\", flags=re.UNICODE)\n    \n    return emoji_pattern.sub(r'', text)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:38:50.866948Z","iopub.execute_input":"2024-09-21T12:38:50.867359Z","iopub.status.idle":"2024-09-21T12:38:50.873375Z","shell.execute_reply.started":"2024-09-21T12:38:50.867321Z","shell.execute_reply":"2024-09-21T12:38:50.872254Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"text = \"I am happy 😊, but a little tired 😴\"\nclean_text = remove_emojis(text)\nclean_text","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:39:23.849960Z","iopub.execute_input":"2024-09-21T12:39:23.850880Z","iopub.status.idle":"2024-09-21T12:39:23.860168Z","shell.execute_reply.started":"2024-09-21T12:39:23.850784Z","shell.execute_reply":"2024-09-21T12:39:23.858709Z"},"trusted":true},"execution_count":127,"outputs":[{"execution_count":127,"output_type":"execute_result","data":{"text/plain":"'I am happy , but a little tired '"},"metadata":{}}]},{"cell_type":"code","source":"# Replacing emojis with their meaning\n\nimport emoji\n\ndef replace_emojis(text):\n    return emoji.demojize(text, delimiters=(\"\", \"\"))\n\n# Example usage\ntext = \"I am happy 😊, but a little tired 😴\"\nprocessed_text = replace_emojis(text)\nprint(processed_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:41:25.190978Z","iopub.execute_input":"2024-09-21T12:41:25.191374Z","iopub.status.idle":"2024-09-21T12:41:25.198631Z","shell.execute_reply.started":"2024-09-21T12:41:25.191338Z","shell.execute_reply":"2024-09-21T12:41:25.197204Z"},"trusted":true},"execution_count":132,"outputs":[{"name":"stdout","text":"I am happy smiling_face_with_smiling_eyes, but a little tired sleeping_face\n","output_type":"stream"}]},{"cell_type":"code","source":"# Using a sentiment map to replace emoji with sentiments\nemoji_sentiment_map = {\n    \"😊\": \"happy\",\n    \"😴\": \"tired\",\n    \"😡\": \"angry\",\n    \"😂\": \"laughing\",\n    \"😢\": \"sad\"\n}\n\ndef convert_emojis_to_sentiment(text):\n    for emoji_char, sentiment in emoji_sentiment_map.items():\n        text = text.replace(emoji_char, sentiment)\n    return text\n\n# Example usage\ntext = \"I am 😊, but a little 😴\"\nprocessed_text = convert_emojis_to_sentiment(text)\nprint(processed_text) \n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:41:40.766072Z","iopub.execute_input":"2024-09-21T12:41:40.767340Z","iopub.status.idle":"2024-09-21T12:41:40.774669Z","shell.execute_reply.started":"2024-09-21T12:41:40.767279Z","shell.execute_reply":"2024-09-21T12:41:40.773514Z"},"trusted":true},"execution_count":135,"outputs":[{"name":"stdout","text":"I am happy, but a little tired\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 9. Tokenization","metadata":{}},{"cell_type":"markdown","source":"Tokenization is the process of breaking down text into smaller units called \"tokens.\" These tokens can be words, subwords, or characters, depending on the approach. Tokenization is a crucial step in text preprocessing because it transforms unstructured text data into a format that can be easily understood and analyzed by machine learning models.","metadata":{}},{"cell_type":"markdown","source":"### Tokenization Tools:\n#### NLTK (Natural Language Toolkit): \nProvides simple word and sentence tokenizers.\n#### SpaCy: \nA more advanced NLP library that handles tokenization efficiently.\n#### Hugging Face Tokenizers: \nDesigned for fast, memory-efficient tokenization in deep learning models like BERT, GPT, etc.","metadata":{}},{"cell_type":"code","source":"# using python split function\n\n# word\ntext = \"Tokenization is important for NLP tasks.\"\nprint(text.split())\n\n# sentence\ntext = \"Tokenization is the first step in NLP. It splits text into smaller parts.\"\nprint(text.split(\".\"))","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:50:37.540980Z","iopub.execute_input":"2024-09-21T12:50:37.541407Z","iopub.status.idle":"2024-09-21T12:50:37.547996Z","shell.execute_reply.started":"2024-09-21T12:50:37.541365Z","shell.execute_reply":"2024-09-21T12:50:37.546756Z"},"trusted":true},"execution_count":143,"outputs":[{"name":"stdout","text":"['Tokenization', 'is', 'important', 'for', 'NLP', 'tasks.']\n['Tokenization is the first step in NLP', ' It splits text into smaller parts', '']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It has it's own limitation. As seen above, the **tasks.** is now a token and it will be different from the next **tasks** token. This creates a issue. ","metadata":{}},{"cell_type":"code","source":"# Regular Expressions\n\nimport re\n\ndef tokenize(text):\n    # Regular expression for matching words, numbers, and punctuation\n    pattern = r'\\w+|[^\\w\\s]'\n    return re.findall(pattern, text)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:54:15.006412Z","iopub.execute_input":"2024-09-21T12:54:15.006844Z","iopub.status.idle":"2024-09-21T12:54:15.013772Z","shell.execute_reply.started":"2024-09-21T12:54:15.006803Z","shell.execute_reply":"2024-09-21T12:54:15.012598Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"['Tokenization', 'is', 'hard', '!', 'Let', \"'\", 's', 'break', 'it', 'down', ':', '48', '+', '20', '=', '68', '.']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Example\ntext = \"Tokenization is hard! Let's break it down: 48+20 = 68.\"\ntokens = tokenize(text)\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:54:27.143743Z","iopub.execute_input":"2024-09-21T12:54:27.144187Z","iopub.status.idle":"2024-09-21T12:54:27.150596Z","shell.execute_reply.started":"2024-09-21T12:54:27.144145Z","shell.execute_reply":"2024-09-21T12:54:27.149499Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stdout","text":"['Tokenization', 'is', 'hard', '!', 'Let', \"'\", 's', 'break', 'it', 'down', ':', '48', '+', '20', '=', '68', '.']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Using NLTK","metadata":{}},{"cell_type":"code","source":"# Word tokenization\n\nfrom nltk.tokenize import word_tokenize\n\ntext = \"Tokenization is important for NLP tasks.\"\ntokens = word_tokenize(text)\nprint(tokens) ","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:45:18.580073Z","iopub.execute_input":"2024-09-21T12:45:18.581796Z","iopub.status.idle":"2024-09-21T12:45:18.587981Z","shell.execute_reply.started":"2024-09-21T12:45:18.581727Z","shell.execute_reply":"2024-09-21T12:45:18.586925Z"},"trusted":true},"execution_count":138,"outputs":[{"name":"stdout","text":"['Tokenization', 'is', 'important', 'for', 'NLP', 'tasks', '.']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Sentence tokenization\n\nfrom nltk.tokenize import sent_tokenize\n\ntext = \"Tokenization is the first step in NLP. It splits text into smaller parts.\"\nsentences = sent_tokenize(text)\n\nsentences\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:46:56.475761Z","iopub.execute_input":"2024-09-21T12:46:56.476808Z","iopub.status.idle":"2024-09-21T12:46:56.484124Z","shell.execute_reply.started":"2024-09-21T12:46:56.476760Z","shell.execute_reply":"2024-09-21T12:46:56.482916Z"},"trusted":true},"execution_count":141,"outputs":[{"execution_count":141,"output_type":"execute_result","data":{"text/plain":"['Tokenization is the first step in NLP.',\n 'It splits text into smaller parts.']"},"metadata":{}}]},{"cell_type":"markdown","source":"### Using Spacy\nThis is probably the best tokenization library. ","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:58:55.140594Z","iopub.execute_input":"2024-09-21T12:58:55.141065Z","iopub.status.idle":"2024-09-21T12:58:56.006470Z","shell.execute_reply.started":"2024-09-21T12:58:55.141015Z","shell.execute_reply":"2024-09-21T12:58:56.005344Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"text = \"Tokenization is the first step in NLP.\"\ntok1 = nlp(text)\n\nfor tok in tok1:\n    print(tok)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T12:59:03.736454Z","iopub.execute_input":"2024-09-21T12:59:03.736901Z","iopub.status.idle":"2024-09-21T12:59:03.754746Z","shell.execute_reply.started":"2024-09-21T12:59:03.736842Z","shell.execute_reply":"2024-09-21T12:59:03.753138Z"},"trusted":true},"execution_count":150,"outputs":[{"name":"stdout","text":"Tokenization\nis\nthe\nfirst\nstep\nin\nNLP\n.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 10. Stemming","metadata":{}},{"cell_type":"markdown","source":"Stemming is a text normalization technique in Natural Language Processing (NLP) that **reduces words to their base or root form.** The root form (called a \"*stem*\") may not always be a valid word but still represents the core meaning. \n\nFor example, words like \"running,\" \"runs,\" and \"ran\" might all be reduced to \"**run**\" in their stemmed form.\n\n- Most widely used in **Information Retrieval System**, like google search engienes. ","metadata":{}},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\n\nps = PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2024-09-21T13:07:04.286608Z","iopub.execute_input":"2024-09-21T13:07:04.287046Z","iopub.status.idle":"2024-09-21T13:07:04.292163Z","shell.execute_reply.started":"2024-09-21T13:07:04.287006Z","shell.execute_reply":"2024-09-21T13:07:04.290947Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"code","source":"def stem_words(text):\n    return \" \".join([ps.stem(word) for word in text.split()])","metadata":{"execution":{"iopub.status.busy":"2024-09-21T13:10:17.207321Z","iopub.execute_input":"2024-09-21T13:10:17.207755Z","iopub.status.idle":"2024-09-21T13:10:17.213478Z","shell.execute_reply.started":"2024-09-21T13:10:17.207709Z","shell.execute_reply":"2024-09-21T13:10:17.212211Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"# words = [\"running\", \"runs\", \"runner\", \"easily\", \"fairly\"]\nsentence1 = \"Running man runs like he has runned for years.\"\n\nprint(stem_words(sentence1))","metadata":{"execution":{"iopub.status.busy":"2024-09-21T13:11:03.257198Z","iopub.execute_input":"2024-09-21T13:11:03.257622Z","iopub.status.idle":"2024-09-21T13:11:03.264531Z","shell.execute_reply.started":"2024-09-21T13:11:03.257582Z","shell.execute_reply":"2024-09-21T13:11:03.263003Z"},"trusted":true},"execution_count":165,"outputs":[{"name":"stdout","text":"run man run like he ha run for years.\n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\n\nstemmer = SnowballStemmer(\"english\", True)\ntext = \"There is nothing either good or bad but thinking makes it so.\"\nwords = word_tokenize(text)\nstemmed_words = [stemmer.stem(word) for word in words]\n\nprint(\"Original:\", text)\nprint(\"Tokenized:\", words)\nprint(\"Stemmed:\", stemmed_words)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T13:20:25.466316Z","iopub.execute_input":"2024-09-21T13:20:25.466777Z","iopub.status.idle":"2024-09-21T13:20:25.475606Z","shell.execute_reply.started":"2024-09-21T13:20:25.466734Z","shell.execute_reply":"2024-09-21T13:20:25.474452Z"},"trusted":true},"execution_count":171,"outputs":[{"name":"stdout","text":"Original: There is nothing either good or bad but thinking makes it so.\nTokenized: ['There', 'is', 'nothing', 'either', 'good', 'or', 'bad', 'but', 'thinking', 'makes', 'it', 'so', '.']\nStemmed: ['there', 'is', 'noth', 'either', 'good', 'or', 'bad', 'but', 'think', 'make', 'it', 'so', '.']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The output os the stemming may not be a english word always. \n- If speed matters, and we don't have to show output to users, go with stemming. Else, **lemmatization** is the way to go. ","metadata":{}},{"cell_type":"markdown","source":"# 11. Lemmatization","metadata":{}},{"cell_type":"markdown","source":"Lemmatization is another text normalization technique in Natural Language Processing (NLP) that reduces words to their base or dictionary form, called a lemma. Unlike stemming, lemmatization considers the context of a word, using linguistic knowledge such as the word's part of speech, and ensures the result is a valid word in the language.\n\nFor example, words like \"am,\" \"is,\" \"are,\" and \"were\" are all lemmatized to \"be\" because they are different forms of the same verb.","metadata":{}},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk import word_tokenize, pos_tag\n \nnltk.download()\n    \ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:         \n        return wordnet.NOUN\n       \ndef lemmatize_passage(text):\n    words = word_tokenize(text)\n    pos_tags = pos_tag(words)\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n    lemmatized_sentence = ' '.join(lemmatized_words)\n    return lemmatized_sentence\n \ntext = \"There is nothing either good or bad but thinking makes it so.\"\nresult = lemmatize_passage(text)\n \nprint(\"Original:\", text)\nprint(\"Tokenized:\", word_tokenize(text))\nprint(\"Lemmatized:\", result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nprint(nltk.data.path)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T13:27:12.110998Z","iopub.execute_input":"2024-09-21T13:27:12.111442Z","iopub.status.idle":"2024-09-21T13:27:12.117312Z","shell.execute_reply.started":"2024-09-21T13:27:12.111404Z","shell.execute_reply":"2024-09-21T13:27:12.115955Z"},"trusted":true},"execution_count":180,"outputs":[{"name":"stdout","text":"['/root/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nfrom nltk.stem import WordNetLemmatizer\n\n# Create a lemmatizer object\nlemmatizer = WordNetLemmatizer()\n\n# Example words\nwords = [\"running\", \"ran\", \"better\", \"feet\", \"geese\", \"are\"]\n\n# Apply lemmatization\nlemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n\nprint(lemmatized_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk import word_tokenize, pos_tag\n \ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:         \n        return wordnet.NOUN\n       \ndef lemmatize_passage(text):\n    words = word_tokenize(text)\n    pos_tags = pos_tag(words)\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n    lemmatized_sentence = ' '.join(lemmatized_words)\n    return lemmatized_sentence\n \ntext = \"There is nothing either good or bad but thinking makes it so.\"\nresult = lemmatize_passage(text)\n \nprint(\"Original:\", text)\nprint(\"Tokenized:\", word_tokenize(text))\nprint(\"Lemmatized:\", result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"Text preprocessing is a crucial step in Natural Language Processing (NLP) that significantly influences the quality and effectiveness of models. It involves a series of techniques aimed at transforming raw text into a structured format suitable for analysis and modeling.","metadata":{}}]}